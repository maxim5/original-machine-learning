{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This tutorial covers your simplest neural network: a multilayer perceptron (MLP)\n",
    "# Also known as feedforward neural network.\n",
    "# We will learn to classify MNIST handwritten digit images into their correct label (0-9).\n",
    "\n",
    "from IPython.display import Image\n",
    "# First, let's load our data and take a look!\n",
    "import pickle\n",
    "\n",
    "# Load our data \n",
    "# Download and unzip pickled version from here: http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = pickle.load(open('data/mnist.pkl', 'r'))\n",
    "print \"Shapes:\"\n",
    "print train_x.shape, train_y.shape\n",
    "print valid_x.shape, valid_y.shape\n",
    "print test_x.shape, test_y.shape\n",
    "\n",
    "print \"--------------\"\n",
    "print \"Example input:\"\n",
    "print train_x[0]\n",
    "print \"Example label:\"\n",
    "print train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Show example images - using tile_raster_images helper function from OpenDeep to get 28x28 image from 784 array.\n",
    "from utils import tile_raster_images\n",
    "from PIL import Image as pil_img\n",
    "\n",
    "input_images = train_x[:25]\n",
    "im = pil_img.fromarray(\n",
    "    tile_raster_images(input_images, \n",
    "                       img_shape=(28, 28), \n",
    "                       tile_shape=(1, 25),\n",
    "                       tile_spacing=(1, 1))\n",
    ")\n",
    "im.save(\"some_mnist_numbers.png\")\n",
    "Image(filename=\"some_mnist_numbers.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cool, now we know a little about the input data, let's design the MLP to work with it!\n",
    "# An MLP looks like this: input -> hiddens -> output classification\n",
    "# Each stage is just a matrix multiplication with a nonlinear function applied after.\n",
    "\n",
    "# Your basic Theano imports.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "# Inputs are matrices where rows are examples and columns are pixels - so create a symbolic Theano matrix.\n",
    "x = T.matrix('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now let's start building the equation for our MLP!\n",
    "\n",
    "# The first transformation is the input x -> hidden layer h.\n",
    "# We defined this transformation with h = tanh(x.dot(W_x) + b_h)\n",
    "# where the learnable model parameters are W_x and b_h.\n",
    "\n",
    "# Therefore, we will need a weights matrix W_x and a bias vector b_h.\n",
    "# W_x has shape (input_size, hidden_size) and b_h has shape (hidden_size,).\n",
    "# Initialization is important in deep learning; we want something random so the model doesn't get stuck early.\n",
    "# Many papers in this subject, but for now we will just use a normal distribution with mean=0 and std=0.05.\n",
    "# Another good option for tanh layers is to use a uniform distribution with interval +- sqrt(6/sum(shape)).\n",
    "# These are hyperparameters to play with.\n",
    "# Bias starting as zero is fine.\n",
    "import numpy.random as rng\n",
    "W_x = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(28*28, 500)), dtype=theano.config.floatX)\n",
    "b_h = numpy.zeros(shape=(500,), dtype=theano.config.floatX)\n",
    "\n",
    "# To update a variable used in an equation (for example, while learning), \n",
    "# Theano needs it to be in a special wrapper called a shared variable.\n",
    "# These are the model parameters for our first hidden layer!\n",
    "W_x = theano.shared(W_x, name=\"W_x\")\n",
    "b_h = theano.shared(b_h, name=\"b_h\")\n",
    "\n",
    "# Now, we can finally write the equation to give our symbolic hidden layer h!\n",
    "h = T.tanh(\n",
    "    T.dot(x, W_x) + b_h\n",
    ")\n",
    "\n",
    "# Side note - if we used softmax instead of tanh for the activation, this would be performing logistic regression!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We have the hidden layer h, let's put that softmax layer on top for classification output y!\n",
    "\n",
    "# Same deal as before, the transformation is defined as:\n",
    "# y = softmax(h.dot(W_h) + b_y)\n",
    "# where the learnable parameters are W_h and b_y.\n",
    "# W_h has shape (hidden_size, output_size) and b_y has shape (output_size,).\n",
    "\n",
    "# We will use the same random initialization strategy as before.\n",
    "W_h = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(500, 10)), dtype=theano.config.floatX)\n",
    "b_y = numpy.zeros(shape=(10,), dtype=theano.config.floatX)\n",
    "# Don't forget to make them shared variables!\n",
    "W_h = theano.shared(W_h, name=\"W_h\")\n",
    "b_y = theano.shared(b_y, name=\"b_y\")\n",
    "\n",
    "# Now write the equation for the output!\n",
    "y = T.nnet.softmax(\n",
    "    T.dot(h, W_h) + b_y\n",
    ")\n",
    "\n",
    "# The output (due to softmax) is a vector of class probabilities.\n",
    "# To get the output class 'guess' from the model, just take the index of the highest probability!\n",
    "y_hat = T.argmax(y, axis=1)\n",
    "\n",
    "# That's everything! Just four model parameters and one input variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The variable y_hat represents the output of running our model, but we need a cost function to use for training.\n",
    "# For a softmax (probability) output, we want to maximize the likelihood of P(Y=y|X).\n",
    "# This means we want to minimize the negative log-likelihood cost! (For a primer, see machine learning Coursera.)\n",
    "\n",
    "# Cost functions always need the truth outputs to compare against (this is supervised learning).\n",
    "# From before, we saw the labels were a vector of ints - so let's make a symbolic variable for this!\n",
    "correct_labels = T.ivector(\"labels\")  # integer vector\n",
    "\n",
    "# Now we can compare our output probability from y with the true labels.\n",
    "# Because the labels are integers, we will want to make an indexing mask to pick out the probabilities\n",
    "# our model thought was the likelihood of the correct label.\n",
    "log_likelihood = T.log(y)[T.arange(correct_labels.shape[0]), correct_labels]\n",
    "# We use mean instead of sum to be less dependent on batch size (better for flexibility)\n",
    "cost = -T.mean(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Easiest way to train neural nets is with Stochastic Gradient Descent\n",
    "# This takes each example, calculates the gradient, and changes the model parameters a small amount\n",
    "# in the direction of the gradient.\n",
    "\n",
    "# Fancier add-ons to stochastic gradient descent will reduce the learning rate over time, add a momentum\n",
    "# factor to the parameters, etc.\n",
    "\n",
    "# Before we can start training, we need to know what the gradients are.\n",
    "# Luckily we don't have to do any math! Theano has symbolic auto-differentiation which means it can\n",
    "# calculate the gradients for arbitrary equations with respect to a cost and parameters.\n",
    "parameters = [W_x, b_h, W_h, b_y]\n",
    "gradients = T.grad(cost, parameters)\n",
    "# Now gradients contains the list of derivatives: [d_cost/d_W_x, d_cost/d_b_h, d_cost/d_W_h, d_cost/d_b_y]\n",
    "\n",
    "# One last thing we need to do before training is to use these gradients to update the parameters!\n",
    "# Remember how parameters are shared variables? Well, Theano uses something called updates\n",
    "# which are just pairs of (shared_variable, new_variable_expression) to change its value.\n",
    "# So, let's create these updates to show how we change the parameter values during training with gradients!\n",
    "# We use a learning rate to make small steps over time.\n",
    "learning_rate = 0.01\n",
    "train_updates = [(param, param - learning_rate*gradient) for param, gradient in zip(parameters, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Now we can create a Theano function that takes in real inputs and trains our model.\n",
    "f_train = theano.function(inputs=[x, correct_labels], outputs=cost, updates=train_updates, allow_input_downcast=True)\n",
    "\n",
    "# For testing purposes, we don't want to use updates to change the parameters - so create a separate function!\n",
    "# We also care more about the output guesses, so let's return those instead of the cost.\n",
    "# error = sum(T.neq(y_hat, correct_labels))/float(y_hat.shape[0])\n",
    "f_test = theano.function(inputs=[x], outputs=y_hat, allow_input_downcast=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Our training can begin!\n",
    "# The two hyperparameters we have for this part are minibatch size (how many examples to process in parallel)\n",
    "# and the total number of passes over all examples (epochs).\n",
    "batch_size = 100\n",
    "epochs = 30\n",
    "\n",
    "# Given our batch size, compute how many batches we can fit into each data set\n",
    "train_batches = len(train_x) / batch_size\n",
    "valid_batches = len(valid_x) / batch_size\n",
    "test_batches = len(test_x) / batch_size\n",
    "\n",
    "# Our main training loop!\n",
    "for epoch in range(epochs):\n",
    "    print epoch+1, \":\",\n",
    "    \n",
    "    train_costs = []\n",
    "    train_accuracy = []\n",
    "    for i in range(train_batches):\n",
    "        # Grab our minibatch of examples from the whole train set.\n",
    "        batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = train_y[i*batch_size:(i+1)*batch_size]\n",
    "        # Compute the costs from the train function (which also updates the parameters)\n",
    "        costs = f_train(batch_x, batch_labels)\n",
    "        # Compute the predictions from the test function (which does not update parameters)\n",
    "        preds = f_test(batch_x)\n",
    "        # Compute the accuracy of our predictions against the correct batch labels\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        train_costs.append(costs)\n",
    "        train_accuracy.append(acc)\n",
    "    # Show the mean cost and accuracy across minibatches (the entire train set!)\n",
    "    print \"cost:\", numpy.mean(train_costs), \"\\ttrain:\", str(numpy.mean(train_accuracy)*100)+\"%\",\n",
    "    \n",
    "    valid_accuracy = []\n",
    "    for i in range(valid_batches):\n",
    "        batch_x = valid_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = valid_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        preds = f_test(batch_x)\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        valid_accuracy.append(acc)\n",
    "    print \"\\tvalid:\", str(numpy.mean(valid_accuracy)*100)+\"%\",\n",
    "    \n",
    "    test_accuracy = []\n",
    "    for i in range(test_batches):\n",
    "        batch_x = test_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = test_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        preds = f_test(batch_x)\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        test_accuracy.append(acc)\n",
    "    print \"\\ttest:\", str(numpy.mean(test_accuracy)*100)+\"%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
