{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Shapes:\n",
      "(50000, 784) (50000,)\n",
      "(10000, 784) (10000,)\n",
      "(10000, 784) (10000,)\n",
      "--------------\n",
      "Example input:\n",
      "[ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.01171875\n",
      "  0.0703125   0.0703125   0.0703125   0.4921875   0.53125     0.68359375\n",
      "  0.1015625   0.6484375   0.99609375  0.96484375  0.49609375  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.1171875   0.140625    0.3671875\n",
      "  0.6015625   0.6640625   0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.87890625  0.671875    0.98828125  0.9453125   0.76171875\n",
      "  0.25        0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.19140625\n",
      "  0.9296875   0.98828125  0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.98828125  0.98046875  0.36328125  0.3203125\n",
      "  0.3203125   0.21875     0.15234375  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.0703125   0.85546875  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.7734375   0.7109375   0.96484375  0.94140625\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.3125      0.609375\n",
      "  0.41796875  0.98828125  0.98828125  0.80078125  0.04296875  0.\n",
      "  0.16796875  0.6015625   0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.0546875   0.00390625  0.6015625   0.98828125  0.3515625   0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.54296875  0.98828125  0.7421875   0.0078125   0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.04296875  0.7421875   0.98828125  0.2734375   0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.13671875  0.94140625  0.87890625\n",
      "  0.625       0.421875    0.00390625  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.31640625\n",
      "  0.9375      0.98828125  0.98828125  0.46484375  0.09765625  0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.17578125  0.7265625   0.98828125  0.98828125  0.5859375   0.10546875\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.0625      0.36328125  0.984375    0.98828125\n",
      "  0.73046875  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.97265625\n",
      "  0.98828125  0.97265625  0.25        0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.1796875   0.5078125   0.71484375\n",
      "  0.98828125  0.98828125  0.80859375  0.0078125   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.15234375  0.578125    0.89453125  0.98828125\n",
      "  0.98828125  0.98828125  0.9765625   0.7109375   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.09375     0.4453125   0.86328125  0.98828125  0.98828125\n",
      "  0.98828125  0.98828125  0.78515625  0.3046875   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.08984375  0.2578125   0.83203125  0.98828125  0.98828125  0.98828125\n",
      "  0.98828125  0.7734375   0.31640625  0.0078125   0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.0703125\n",
      "  0.66796875  0.85546875  0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.76171875  0.3125      0.03515625  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.21484375\n",
      "  0.671875    0.8828125   0.98828125  0.98828125  0.98828125  0.98828125\n",
      "  0.953125    0.51953125  0.04296875  0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.53125     0.98828125  0.98828125  0.98828125  0.828125    0.52734375\n",
      "  0.515625    0.0625      0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.        ]\n",
      "Example label:\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "import pickle\n",
    "\n",
    "# Download and unzip pickled version from here: http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = pickle.load(open('data/mnist.pkl', 'r'))\n",
    "print \"Shapes:\"\n",
    "print train_x.shape, train_y.shape\n",
    "print valid_x.shape, valid_y.shape\n",
    "print test_x.shape, test_y.shape\n",
    "\n",
    "print \"--------------\"\n",
    "print \"Example input:\"\n",
    "print train_x[0]\n",
    "print \"Example label:\"\n",
    "print train_y[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAtQAAAAcCAAAAABkYnfcAAAQtklEQVR4nO1beVxV5dZ+EgQHUJFS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\nQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show example images - using tile_raster_images helper function from OpenDeep to get 28x28 image from 784 array.\n",
    "from utils import tile_raster_images\n",
    "from PIL import Image as pil_img\n",
    "\n",
    "input_images = train_x[:25]\n",
    "im = pil_img.fromarray(\n",
    "    tile_raster_images(input_images, \n",
    "                       img_shape=(28, 28), \n",
    "                       tile_shape=(1, 25),\n",
    "                       tile_spacing=(1, 1))\n",
    ")\n",
    "im.save(\"some_mnist_numbers.png\")\n",
    "Image(filename=\"some_mnist_numbers.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Your basic Theano imports.\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "\n",
    "x = T.matrix('x')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the hidden layer from the input\n",
    "import numpy\n",
    "import numpy.random as rng\n",
    "\n",
    "i = numpy.sqrt(6. / (784+500))\n",
    "# W_x = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(28*28, 500)), dtype=theano.config.floatX)\n",
    "W_x = numpy.asarray(rng.uniform(low=-i, high=i, size=(28*28, 500)), dtype=theano.config.floatX)\n",
    "b_h = numpy.zeros(shape=(500,), dtype=theano.config.floatX)\n",
    "\n",
    "W_x = theano.shared(W_x, name=\"W_x\")\n",
    "b_h = theano.shared(b_h, name=\"b_h\")\n",
    "\n",
    "h = T.tanh(\n",
    "    T.dot(x, W_x) + b_h\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the output class probabilities from the hidden layer\n",
    "i = numpy.sqrt(6. / (510))\n",
    "# W_h = numpy.asarray(rng.normal(loc=0.0, scale=.05, size=(500, 10)), dtype=theano.config.floatX)\n",
    "W_h = numpy.asarray(rng.uniform(low=-i, high=i, size=(500, 10)), dtype=theano.config.floatX)\n",
    "b_y = numpy.zeros(shape=(10,), dtype=\"float32\")\n",
    "\n",
    "W_h = theano.shared(W_h, name=\"W_h\")\n",
    "b_y = theano.shared(b_y, name=\"b_y\")\n",
    "\n",
    "y = T.nnet.softmax(\n",
    "    T.dot(h, W_h) + b_y\n",
    ")\n",
    "\n",
    "# The actual predicted label\n",
    "y_hat = T.argmax(y, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find cost compared to correct labels\n",
    "correct_labels = T.ivector(\"labels\")\n",
    "\n",
    "log_likelihood = T.log(y)[T.arange(correct_labels.shape[0]), correct_labels]\n",
    "cost = -T.mean(log_likelihood)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute gradient updates for the parameters\n",
    "parameters = [W_x, b_h, W_h, b_y]\n",
    "gradients = T.grad(cost, parameters)\n",
    "\n",
    "learning_rate = 0.01\n",
    "train_updates = [(param, param - learning_rate*gradient) for param, gradient in zip(parameters, gradients)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compile function for training (changes parameters via updates) and testing (no updates)\n",
    "f_train = theano.function(\n",
    "    inputs=[x, correct_labels], \n",
    "    outputs=cost, \n",
    "    updates=train_updates, \n",
    "    allow_input_downcast=True\n",
    ")\n",
    "\n",
    "f_test = theano.function(\n",
    "    inputs=[x], \n",
    "    outputs=y_hat, \n",
    "    allow_input_downcast=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 : cost: 0.910093 \ttrain: 79.788% \tvalid: 88.05% \ttest: 87.49%\n",
      "2 : cost: 0.492869 \ttrain: 87.558% \tvalid: 89.6% \ttest: 88.99%\n",
      "3 : cost: 0.419445 \ttrain: 88.848% \tvalid: 90.26% \ttest: 89.9%\n",
      "saving filters...\n",
      "4 : cost: 0.38414 \ttrain: 89.536% \tvalid: 90.67% \ttest: 90.37%\n",
      "5 : cost: 0.362308 \ttrain: 89.99% \tvalid: 91.1% \ttest: 90.69%\n",
      "6 : cost: 0.346996 \ttrain: 90.424% \tvalid: 91.34% \ttest: 90.95%\n",
      "saving filters...\n",
      "7 : cost: 0.335397 \ttrain: 90.724% \tvalid: 91.51% \ttest: 91.15%\n",
      "8 : cost: 0.326137 \ttrain: 90.982% \tvalid: 91.76% \ttest: 91.42%\n",
      "9 : cost: 0.318454 \ttrain: 91.17% \tvalid: 91.89% \ttest: 91.65%\n",
      "saving filters...\n",
      "10 : cost: 0.311887 \ttrain: 91.354% \tvalid: 92.02% \ttest: 91.8%\n",
      "11 : cost: 0.306141 \ttrain: 91.554% \tvalid: 92.08% \ttest: 91.98%\n",
      "12 : cost: 0.301014 \ttrain: 91.684% \tvalid: 92.2% \ttest: 92.04%\n",
      "saving filters...\n",
      "13 : cost: 0.296363 \ttrain: 91.804% \tvalid: 92.22% \ttest: 92.08%\n",
      "14 : cost: 0.292087 \ttrain: 91.922% \tvalid: 92.22% \ttest: 92.17%\n",
      "15 : cost: 0.288107 \ttrain: 92.034% \tvalid: 92.31% \ttest: 92.23%\n",
      "saving filters...\n",
      "16 : cost: 0.284364 \ttrain: 92.126% \tvalid: 92.35% \ttest: 92.32%\n",
      "17 : cost: 0.280813 \ttrain: 92.228% \tvalid: 92.4% \ttest: 92.39%\n",
      "18 : cost: 0.277417 \ttrain: 92.332% \tvalid: 92.45% \ttest: 92.44%\n",
      "saving filters...\n",
      "19 : cost: 0.274148 \ttrain: 92.442% \tvalid: 92.52% \ttest: 92.48%\n",
      "20 : cost: 0.270981 \ttrain: 92.53% \tvalid: 92.6% \ttest: 92.57%\n",
      "21 : cost: 0.267898 \ttrain: 92.618% \tvalid: 92.64% \ttest: 92.67%\n",
      "saving filters...\n",
      "22 : cost: 0.264884 \ttrain: 92.734% \tvalid: 92.73% \ttest: 92.72%\n",
      "23 : cost: 0.261926 \ttrain: 92.814% \tvalid: 92.85% \ttest: 92.8%\n",
      "24 : cost: 0.259013 \ttrain: 92.89% \tvalid: 92.93% \ttest: 92.93%\n",
      "saving filters...\n",
      "25 : cost: 0.256139 \ttrain: 92.984% \tvalid: 93.05% \ttest: 92.95%\n",
      "26 : cost: 0.253295 \ttrain: 93.068% \tvalid: 93.16% \ttest: 92.98%\n",
      "27 : cost: 0.250478 \ttrain: 93.148% \tvalid: 93.22% \ttest: 93.08%\n",
      "saving filters...\n",
      "28 : cost: 0.247683 \ttrain: 93.214% \tvalid: 93.29% \ttest: 93.14%\n",
      "29 : cost: 0.244908 \ttrain: 93.292% \tvalid: 93.42% \ttest: 93.14%\n",
      "30 : cost: 0.24215 \ttrain: 93.374% \tvalid: 93.53% \ttest: 93.21%\n",
      "saving filters...\n",
      "31 : cost: 0.239409 \ttrain: 93.462% \tvalid: 93.61% \ttest: 93.26%\n",
      "32 : cost: 0.236685 \ttrain: 93.538% \tvalid: 93.72% \ttest: 93.36%\n",
      "33 : cost: 0.233977 \ttrain: 93.642% \tvalid: 93.79% \ttest: 93.39%\n",
      "saving filters...\n",
      "34 : cost: 0.231286 \ttrain: 93.712% \tvalid: 93.86% \ttest: 93.43%\n",
      "35 : cost: 0.228612 \ttrain: 93.782% \tvalid: 93.94% \ttest: 93.47%\n",
      "36 : cost: 0.225956 \ttrain: 93.858% \tvalid: 94.06% \ttest: 93.55%\n",
      "saving filters...\n",
      "37 : cost: 0.223321 \ttrain: 93.916% \tvalid: 94.19% \ttest: 93.62%\n",
      "38 : cost: 0.220706 \ttrain: 94.0% \tvalid: 94.35% \ttest: 93.72%\n",
      "39 : cost: 0.218114 \ttrain: 94.068% \tvalid: 94.43% \ttest: 93.84%\n",
      "saving filters...\n",
      "40 : cost: 0.215546 \ttrain: 94.152% \tvalid: 94.48% \ttest: 93.89%\n",
      "41 : cost: 0.213002 \ttrain: 94.232% \tvalid: 94.54% \ttest: 93.96%\n",
      "42 : cost: 0.210484 \ttrain: 94.286% \tvalid: 94.61% \ttest: 94.05%\n",
      "saving filters...\n",
      "43 : cost: 0.207995 \ttrain: 94.368% \tvalid: 94.69% \ttest: 94.09%\n",
      "44 : cost: 0.205533 \ttrain: 94.46% \tvalid: 94.75% \ttest: 94.15%\n",
      "45 : cost: 0.203102 \ttrain: 94.514% \tvalid: 94.81% \ttest: 94.22%\n",
      "saving filters...\n",
      "46 : cost: 0.2007 \ttrain: 94.576% \tvalid: 94.91% \ttest: 94.29%\n",
      "47 : cost: 0.19833 \ttrain: 94.64% \tvalid: 94.94% \ttest: 94.38%\n",
      "48 : cost: 0.195992 \ttrain: 94.71% \tvalid: 94.99% \ttest: 94.45%\n",
      "saving filters...\n",
      "49 : cost: 0.193687 \ttrain: 94.764% \tvalid: 95.02% \ttest: 94.49%\n",
      "50 : cost: 0.191414 \ttrain: 94.826% \tvalid: 95.06% \ttest: 94.53%\n",
      "51 : cost: 0.189174 \ttrain: 94.872% \tvalid: 95.13% \ttest: 94.56%\n",
      "saving filters...\n",
      "52 : cost: 0.186968 \ttrain: 94.924% \tvalid: 95.18% \ttest: 94.61%\n",
      "53 : cost: 0.184796 \ttrain: 94.982% \tvalid: 95.18% \ttest: 94.65%\n",
      "54 : cost: 0.182657 \ttrain: 95.036% \tvalid: 95.24% \ttest: 94.68%\n",
      "saving filters...\n",
      "55 : cost: 0.180553 \ttrain: 95.088% \tvalid: 95.3% \ttest: 94.68%\n",
      "56 : cost: 0.178481 \ttrain: 95.134% \tvalid: 95.34% \ttest: 94.78%\n",
      "57 : cost: 0.176444 \ttrain: 95.194% \tvalid: 95.38% \ttest: 94.83%\n",
      "saving filters...\n",
      "58 : cost: 0.174439 \ttrain: 95.24% \tvalid: 95.41% \ttest: 94.9%\n",
      "59 : cost: 0.172468 \ttrain: 95.288% \tvalid: 95.4% \ttest: 94.98%\n",
      "60 : cost: 0.170529 \ttrain: 95.358% \tvalid: 95.44% \ttest: 95.01%\n",
      "saving filters...\n",
      "61 : cost: 0.168623 \ttrain: 95.422% \tvalid: 95.5% \ttest: 95.02%\n",
      "62 : cost: 0.166749 \ttrain: 95.47% \tvalid: 95.54% \ttest: 95.05%\n",
      "63 : cost: 0.164906 \ttrain: 95.518% \tvalid: 95.62% \ttest: 95.07%\n",
      "saving filters...\n",
      "64 : cost: 0.163095 \ttrain: 95.568% \tvalid: 95.67% \ttest: 95.13%\n",
      "65 : cost: 0.161314 \ttrain: 95.612% \tvalid: 95.71% \ttest: 95.19%\n",
      "66 : cost: 0.159564 \ttrain: 95.666% \tvalid: 95.72% \ttest: 95.21%\n",
      "saving filters...\n",
      "67 : cost: 0.157843 \ttrain: 95.706% \tvalid: 95.76% \ttest: 95.27%\n",
      "68 : cost: 0.156151 \ttrain: 95.756% \tvalid: 95.83% \ttest: 95.31%\n",
      "69 : cost: 0.154488 \ttrain: 95.806% \tvalid: 95.87% \ttest: 95.31%\n",
      "saving filters...\n",
      "70 : cost: 0.152853 \ttrain: 95.854% \tvalid: 95.88% \ttest: 95.36%\n",
      "71 : cost: 0.151246 \ttrain: 95.904% \tvalid: 95.94% \ttest: 95.38%\n",
      "72 : cost: 0.149666 \ttrain: 95.964% \tvalid: 95.96% \ttest: 95.46%\n",
      "saving filters...\n",
      "73 : cost: 0.148112 \ttrain: 96.016% \tvalid: 95.99% \ttest: 95.5%\n",
      "74 : cost: 0.146585 \ttrain: 96.072% \tvalid: 96.01% \ttest: 95.56%\n",
      "75 : cost: 0.145083 \ttrain: 96.138% \tvalid: 96.05% \ttest: 95.6%\n",
      "saving filters...\n",
      "76 : cost: 0.143606 \ttrain: 96.18% \tvalid: 96.09% \ttest: 95.64%\n",
      "77 : cost: 0.142153 \ttrain: 96.224% \tvalid: 96.1% \ttest: 95.68%\n",
      "78 : cost: 0.140724 \ttrain: 96.254% \tvalid: 96.12% \ttest: 95.74%\n",
      "saving filters...\n",
      "79 : cost: 0.139319 \ttrain: 96.306% \tvalid: 96.14% \ttest: 95.77%\n",
      "80 : cost: 0.137937 \ttrain: 96.348% \tvalid: 96.15% \ttest: 95.77%\n",
      "81 : cost: 0.136577 \ttrain: 96.384% \tvalid: 96.16% \ttest: 95.81%\n",
      "saving filters...\n",
      "82 : cost: 0.13524 \ttrain: 96.42% \tvalid: 96.22% \ttest: 95.86%\n",
      "83 : cost: 0.133923 \ttrain: 96.468% \tvalid: 96.24% \ttest: 95.87%\n",
      "84 : cost: 0.132628 \ttrain: 96.502% \tvalid: 96.3% \ttest: 95.93%\n",
      "saving filters...\n",
      "85 : cost: 0.131354 \ttrain: 96.544% \tvalid: 96.33% \ttest: 95.96%\n",
      "86 : cost: 0.130099 \ttrain: 96.58% \tvalid: 96.36% \ttest: 96.03%\n",
      "87 : cost: 0.128864 \ttrain: 96.608% \tvalid: 96.38% \ttest: 96.07%\n",
      "saving filters...\n",
      "88 : cost: 0.127649 \ttrain: 96.658% \tvalid: 96.42% \ttest: 96.09%\n",
      "89 : cost: 0.126453 \ttrain: 96.688% \tvalid: 96.43% \ttest: 96.1%\n",
      "90 : cost: 0.125275 \ttrain: 96.71% \tvalid: 96.44% \ttest: 96.14%\n",
      "saving filters...\n",
      "91 : cost: 0.124115 \ttrain: 96.742% \tvalid: 96.44% \ttest: 96.16%\n",
      "92 : cost: 0.122972 \ttrain: 96.79% \tvalid: 96.5% \ttest: 96.18%\n",
      "93 : cost: 0.121848 \ttrain: 96.822% \tvalid: 96.51% \ttest: 96.18%\n",
      "saving filters...\n",
      "94 : cost: 0.120739 \ttrain: 96.854% \tvalid: 96.52% \ttest: 96.2%\n",
      "95 : cost: 0.119648 \ttrain: 96.892% \tvalid: 96.53% \ttest: 96.22%\n",
      "96 : cost: 0.118573 \ttrain: 96.93% \tvalid: 96.56% \ttest: 96.26%\n",
      "saving filters...\n",
      "97 : cost: 0.117514 \ttrain: 96.954% \tvalid: 96.58% \ttest: 96.29%\n",
      "98 : cost: 0.11647 \ttrain: 96.97% \tvalid: 96.58% \ttest: 96.3%\n",
      "99 : cost: 0.115442 \ttrain: 96.994% \tvalid: 96.6% \ttest: 96.33%\n",
      "saving filters...\n",
      "100 : cost: 0.114428 \ttrain: 97.022% \tvalid: 96.61% \ttest: 96.33%\n",
      "101 : cost: 0.113429 \ttrain: 97.062% \tvalid: 96.62% \ttest: 96.33%\n",
      "102 : cost: 0.112444 \ttrain: 97.092% \tvalid: 96.61% \ttest: 96.36%\n",
      "saving filters...\n",
      "103 : cost: 0.111474 \ttrain: 97.136% \tvalid: 96.64% \ttest: 96.37%\n",
      "104 : cost: 0.110517 \ttrain: 97.154% \tvalid: 96.69% \ttest: 96.39%\n",
      "105 : cost: 0.109573 \ttrain: 97.18% \tvalid: 96.72% \ttest: 96.4%\n",
      "saving filters...\n",
      "106 : cost: 0.108643 \ttrain: 97.208% \tvalid: 96.73% \ttest: 96.44%\n",
      "107 : cost: 0.107725 \ttrain: 97.246% \tvalid: 96.74% \ttest: 96.45%\n",
      "108 : cost: 0.10682 \ttrain: 97.27% \tvalid: 96.74% \ttest: 96.48%\n",
      "saving filters...\n",
      "109 : cost: 0.105928 \ttrain: 97.288% \tvalid: 96.76% \ttest: 96.5%\n",
      "110 : cost: 0.105048 \ttrain: 97.322% \tvalid: 96.78% \ttest: 96.53%\n",
      "111 : cost: 0.104179 \ttrain: 97.338% \tvalid: 96.8% \ttest: 96.56%\n",
      "saving filters...\n",
      "112 : cost: 0.103322 \ttrain: 97.364% \tvalid: 96.8% \ttest: 96.59%\n",
      "113 : cost: 0.102477 \ttrain: 97.386% \tvalid: 96.8% \ttest: 96.63%\n",
      "114 : cost: 0.101643 \ttrain: 97.41% \tvalid: 96.81% \ttest: 96.64%\n",
      "saving filters...\n",
      "115 : cost: 0.10082 \ttrain: 97.438% \tvalid: 96.82% \ttest: 96.65%\n",
      "116 : cost: 0.100007 \ttrain: 97.46% \tvalid: 96.84% \ttest: 96.66%\n",
      "117 : cost: 0.0992054 \ttrain: 97.476% \tvalid: 96.85% \ttest: 96.67%\n",
      "saving filters...\n",
      "118 : cost: 0.0984139 \ttrain: 97.498% \tvalid: 96.87% \ttest: 96.68%\n",
      "119 : cost: 0.0976325 \ttrain: 97.532% \tvalid: 96.89% \ttest: 96.68%\n",
      "120 : cost: 0.0968612 \ttrain: 97.546% \tvalid: 96.89% \ttest: 96.69%\n",
      "saving filters...\n",
      "121 : cost: 0.0960996 \ttrain: 97.558% \tvalid: 96.9% \ttest: 96.7%\n",
      "122 : cost: 0.0953476 \ttrain: 97.592% \tvalid: 96.91% \ttest: 96.7%\n",
      "123 : cost: 0.094605 \ttrain: 97.612% \tvalid: 96.92% \ttest: 96.72%\n",
      "saving filters...\n",
      "124 : cost: 0.0938718 \ttrain: 97.63% \tvalid: 96.94% \ttest: 96.74%\n",
      "125 : cost: 0.0931474 \ttrain: 97.652% \tvalid: 96.95% \ttest: 96.74%\n",
      "126 : cost: 0.0924321 \ttrain: 97.668% \tvalid: 96.98% \ttest: 96.74%\n",
      "saving filters...\n",
      "127 : cost: 0.0917255 \ttrain: 97.684% \tvalid: 96.99% \ttest: 96.74%\n",
      "128 : cost: 0.0910275 \ttrain: 97.706% \tvalid: 97.0% \ttest: 96.77%\n",
      "129 : cost: 0.0903379 \ttrain: 97.724% \tvalid: 97.01% \ttest: 96.79%\n",
      "saving filters...\n",
      "130 : cost: 0.0896565 \ttrain: 97.736% \tvalid: 97.02% \ttest: 96.79%\n",
      "131 : cost: 0.0889833 \ttrain: 97.756% \tvalid: 97.02% \ttest: 96.79%\n",
      "132 : cost: 0.0883181 \ttrain: 97.772% \tvalid: 97.03% \ttest: 96.8%\n",
      "saving filters...\n",
      "133 : cost: 0.0876607 \ttrain: 97.786% \tvalid: 97.07% \ttest: 96.8%\n",
      "134 : cost: 0.087011 \ttrain: 97.804% \tvalid: 97.08% \ttest: 96.81%\n",
      "135 : cost: 0.0863689 \ttrain: 97.812% \tvalid: 97.09% \ttest: 96.82%\n",
      "saving filters...\n",
      "136 : cost: 0.0857343 \ttrain: 97.824% \tvalid: 97.12% \ttest: 96.83%\n",
      "137 : cost: 0.085107 \ttrain: 97.842% \tvalid: 97.12% \ttest: 96.86%\n",
      "138 : cost: 0.0844868 \ttrain: 97.86% \tvalid: 97.13% \ttest: 96.88%\n",
      "saving filters...\n",
      "139 : cost: 0.0838737 \ttrain: 97.884% \tvalid: 97.12% \ttest: 96.9%\n",
      "140 : cost: 0.0832676 \ttrain: 97.898% \tvalid: 97.11% \ttest: 96.91%\n",
      "141 : cost: 0.0826682 \ttrain: 97.906% \tvalid: 97.11% \ttest: 96.94%\n",
      "saving filters...\n",
      "142 : cost: 0.0820756 \ttrain: 97.916% \tvalid: 97.12% \ttest: 96.93%\n",
      "143 : cost: 0.0814896 \ttrain: 97.93% \tvalid: 97.14% \ttest: 96.93%\n",
      "144 : cost: 0.0809101 \ttrain: 97.948% \tvalid: 97.15% \ttest: 96.96%\n",
      "saving filters...\n",
      "145 : cost: 0.080337 \ttrain: 97.966% \tvalid: 97.15% \ttest: 96.97%\n",
      "146 : cost: 0.0797701 \ttrain: 97.99% \tvalid: 97.16% \ttest: 96.97%\n",
      "147 : cost: 0.0792095 \ttrain: 98.012% \tvalid: 97.18% \ttest: 96.98%\n",
      "saving filters...\n",
      "148 : cost: 0.0786549 \ttrain: 98.03% \tvalid: 97.19% \ttest: 96.98%\n",
      "149 : cost: 0.0781063 \ttrain: 98.058% \tvalid: 97.21% \ttest: 97.0%\n",
      "150 : cost: 0.0775635 \ttrain: 98.074% \tvalid: 97.21% \ttest: 97.01%\n",
      "saving filters...\n",
      "151 : cost: 0.0770266 \ttrain: 98.082% \tvalid: 97.21% \ttest: 97.02%\n",
      "152 : cost: 0.0764954 \ttrain: 98.096% \tvalid: 97.21% \ttest: 97.02%\n",
      "153 : cost: 0.0759698 \ttrain: 98.12% \tvalid: 97.22% \ttest: 97.02%\n",
      "saving filters...\n",
      "154 : cost: 0.0754497 \ttrain: 98.134% \tvalid: 97.22% \ttest: 97.03%\n",
      "155 : cost: 0.0749351 \ttrain: 98.144% \tvalid: 97.22% \ttest: 97.04%\n",
      "156 : cost: 0.0744257 \ttrain: 98.166% \tvalid: 97.24% \ttest: 97.06%\n",
      "saving filters...\n",
      "157 : cost: 0.0739218 \ttrain: 98.184% \tvalid: 97.24% \ttest: 97.06%\n",
      "158 : cost: 0.0734229 \ttrain: 98.208% \tvalid: 97.27% \ttest: 97.07%\n",
      "159 : cost: 0.0729292 \ttrain: 98.224% \tvalid: 97.28% \ttest: 97.07%\n",
      "saving filters...\n",
      "160 : cost: 0.0724405 \ttrain: 98.234% \tvalid: 97.29% \ttest: 97.08%\n",
      "161 : cost: 0.0719567 \ttrain: 98.246% \tvalid: 97.3% \ttest: 97.1%\n",
      "162 : cost: 0.0714778 \ttrain: 98.262% \tvalid: 97.29% \ttest: 97.12%\n",
      "saving filters...\n",
      "163 : cost: 0.0710037 \ttrain: 98.27% \tvalid: 97.3% \ttest: 97.12%\n",
      "164 : cost: 0.0705344 \ttrain: 98.284% \tvalid: 97.32% \ttest: 97.13%\n",
      "165 : cost: 0.0700698 \ttrain: 98.292% \tvalid: 97.31% \ttest: 97.13%\n",
      "saving filters...\n",
      "166 : cost: 0.0696097 \ttrain: 98.306% \tvalid: 97.3% \ttest: 97.14%\n",
      "167 : cost: 0.0691542 \ttrain: 98.314% \tvalid: 97.29% \ttest: 97.15%\n",
      "168 : cost: 0.0687031 \ttrain: 98.328% \tvalid: 97.31% \ttest: 97.16%\n",
      "saving filters...\n",
      "169 : cost: 0.0682565 \ttrain: 98.344% \tvalid: 97.32% \ttest: 97.16%\n",
      "170 : cost: 0.0678142 \ttrain: 98.344% \tvalid: 97.33% \ttest: 97.17%\n",
      "171 : cost: 0.0673761 \ttrain: 98.348% \tvalid: 97.34% \ttest: 97.17%\n",
      "saving filters...\n",
      "172 : cost: 0.0669422 \ttrain: 98.366% \tvalid: 97.34% \ttest: 97.17%\n",
      "173 : cost: 0.0665125 \ttrain: 98.382% \tvalid: 97.33% \ttest: 97.17%\n",
      "174 : cost: 0.0660869 \ttrain: 98.39% \tvalid: 97.33% \ttest: 97.18%\n",
      "saving filters...\n",
      "175 : cost: 0.0656654 \ttrain: 98.406% \tvalid: 97.33% \ttest: 97.18%\n",
      "176 : cost: 0.0652478 \ttrain: 98.418% \tvalid: 97.33% \ttest: 97.16%\n",
      "177 : cost: 0.0648342 \ttrain: 98.43% \tvalid: 97.33% \ttest: 97.17%\n",
      "saving filters...\n",
      "178 : cost: 0.0644244 \ttrain: 98.442% \tvalid: 97.35% \ttest: 97.17%\n",
      "179 : cost: 0.0640184 \ttrain: 98.458% \tvalid: 97.35% \ttest: 97.17%\n",
      "180 : cost: 0.0636162 \ttrain: 98.466% \tvalid: 97.34% \ttest: 97.18%\n",
      "saving filters...\n",
      "181 : cost: 0.0632178 \ttrain: 98.48% \tvalid: 97.34% \ttest: 97.18%\n",
      "182 : cost: 0.0628229 \ttrain: 98.494% \tvalid: 97.37% \ttest: 97.19%\n",
      "183 : cost: 0.0624317 \ttrain: 98.508% \tvalid: 97.37% \ttest: 97.2%\n",
      "saving filters...\n",
      "184 : cost: 0.0620441 \ttrain: 98.53% \tvalid: 97.37% \ttest: 97.21%\n",
      "185 : cost: 0.06166 \ttrain: 98.538% \tvalid: 97.37% \ttest: 97.23%\n",
      "186 : cost: 0.0612793 \ttrain: 98.55% \tvalid: 97.36% \ttest: 97.24%\n",
      "saving filters...\n",
      "187 : cost: 0.0609021 \ttrain: 98.568% \tvalid: 97.36% \ttest: 97.24%\n",
      "188 : cost: 0.0605283 \ttrain: 98.582% \tvalid: 97.36% \ttest: 97.26%\n",
      "189 : cost: 0.0601578 \ttrain: 98.6% \tvalid: 97.37% \ttest: 97.27%\n",
      "saving filters...\n",
      "190 : cost: 0.0597905 \ttrain: 98.606% \tvalid: 97.37% \ttest: 97.28%\n",
      "191 : cost: 0.0594266 \ttrain: 98.62% \tvalid: 97.38% \ttest: 97.29%\n",
      "192 : cost: 0.0590659 \ttrain: 98.624% \tvalid: 97.38% \ttest: 97.3%\n",
      "saving filters...\n",
      "193 : cost: 0.0587082 \ttrain: 98.628% \tvalid: 97.38% \ttest: 97.3%\n",
      "194 : cost: 0.0583538 \ttrain: 98.632% \tvalid: 97.38% \ttest: 97.3%\n",
      "195 : cost: 0.0580024 \ttrain: 98.634% \tvalid: 97.39% \ttest: 97.31%\n",
      "saving filters...\n",
      "196 : cost: 0.057654 \ttrain: 98.64% \tvalid: 97.38% \ttest: 97.3%\n",
      "197 : cost: 0.0573087 \ttrain: 98.644% \tvalid: 97.39% \ttest: 97.31%\n",
      "198 : cost: 0.0569663 \ttrain: 98.658% \tvalid: 97.38% \ttest: 97.33%\n",
      "saving filters...\n",
      "199 : cost: 0.0566269 \ttrain: 98.666% \tvalid: 97.38% \ttest: 97.33%\n",
      "200 : cost: 0.0562904 \ttrain: 98.674% \tvalid: 97.39% \ttest: 97.34%\n",
      "201 : cost: 0.0559567 \ttrain: 98.688% \tvalid: 97.39% \ttest: 97.34%\n",
      "saving filters...\n",
      "202 : cost: 0.0556259 \ttrain: 98.704% \tvalid: 97.39% \ttest: 97.35%\n",
      "203 : cost: 0.0552979 \ttrain: 98.72% \tvalid: 97.4% \ttest: 97.35%\n",
      "204 : cost: 0.0549725 \ttrain: 98.738% \tvalid: 97.41% \ttest: 97.36%\n",
      "saving filters...\n",
      "205 : cost: 0.05465 \ttrain: 98.742% \tvalid: 97.41% \ttest: 97.36%\n",
      "206 : cost: 0.0543301 \ttrain: 98.744% \tvalid: 97.41% \ttest: 97.37%\n",
      "207 : cost: 0.0540128 \ttrain: 98.756% \tvalid: 97.41% \ttest: 97.37%\n",
      "saving filters...\n",
      "208 : cost: 0.0536982 \ttrain: 98.762% \tvalid: 97.42% \ttest: 97.38%\n",
      "209 : cost: 0.0533862 \ttrain: 98.768% \tvalid: 97.43% \ttest: 97.4%\n",
      "210 : cost: 0.0530767 \ttrain: 98.778% \tvalid: 97.44% \ttest: 97.41%\n",
      "saving filters...\n",
      "211 : cost: 0.0527698 \ttrain: 98.786% \tvalid: 97.46% \ttest: 97.41%\n",
      "212 : cost: 0.0524655 \ttrain: 98.792% \tvalid: 97.46% \ttest: 97.41%\n",
      "213 : cost: 0.0521635 \ttrain: 98.804% \tvalid: 97.46% \ttest: 97.41%\n",
      "saving filters...\n",
      "214 : cost: 0.051864 \ttrain: 98.814% \tvalid: 97.46% \ttest: 97.41%\n",
      "215 : cost: 0.0515669 \ttrain: 98.816% \tvalid: 97.46% \ttest: 97.41%\n",
      "216 : cost: 0.0512722 \ttrain: 98.824% \tvalid: 97.46% \ttest: 97.41%\n",
      "saving filters...\n",
      "217 : cost: 0.0509799 \ttrain: 98.838% \tvalid: 97.46% \ttest: 97.4%\n",
      "218 : cost: 0.0506899 \ttrain: 98.842% \tvalid: 97.45% \ttest: 97.4%\n",
      "219 : cost: 0.0504022 \ttrain: 98.852% \tvalid: 97.46% \ttest: 97.41%\n",
      "saving filters...\n",
      "220 : cost: 0.0501168 \ttrain: 98.86% \tvalid: 97.46% \ttest: 97.43%\n",
      "221 : cost: 0.0498336 \ttrain: 98.876% \tvalid: 97.47% \ttest: 97.43%\n",
      "222 : cost: 0.0495527 \ttrain: 98.888% \tvalid: 97.48% \ttest: 97.43%\n",
      "saving filters...\n",
      "223 : cost: 0.0492739 \ttrain: 98.894% \tvalid: 97.49% \ttest: 97.44%\n",
      "224 : cost: 0.0489974 \ttrain: 98.904% \tvalid: 97.5% \ttest: 97.44%\n",
      "225 : cost: 0.048723 \ttrain: 98.914% \tvalid: 97.51% \ttest: 97.44%\n",
      "saving filters...\n",
      "226 : cost: 0.0484507 \ttrain: 98.926% \tvalid: 97.52% \ttest: 97.45%\n",
      "227 : cost: 0.0481805 \ttrain: 98.942% \tvalid: 97.52% \ttest: 97.45%\n",
      "228 : cost: 0.0479125 \ttrain: 98.946% \tvalid: 97.53% \ttest: 97.45%\n",
      "saving filters...\n",
      "229 : cost: 0.0476465 \ttrain: 98.956% \tvalid: 97.54% \ttest: 97.47%\n",
      "230 : cost: 0.0473825 \ttrain: 98.97% \tvalid: 97.54% \ttest: 97.47%\n",
      "231 : cost: 0.0471206 \ttrain: 98.98% \tvalid: 97.54% \ttest: 97.48%\n",
      "saving filters...\n",
      "232 : cost: 0.0468606 \ttrain: 98.99% \tvalid: 97.54% \ttest: 97.5%\n",
      "233 : cost: 0.0466026 \ttrain: 98.996% \tvalid: 97.54% \ttest: 97.5%\n",
      "234 : cost: 0.0463466 \ttrain: 99.004% \tvalid: 97.54% \ttest: 97.51%\n",
      "saving filters...\n",
      "235 : cost: 0.0460925 \ttrain: 99.014% \tvalid: 97.53% \ttest: 97.51%\n",
      "236 : cost: 0.0458403 \ttrain: 99.02% \tvalid: 97.52% \ttest: 97.52%\n",
      "237 : cost: 0.0455901 \ttrain: 99.026% \tvalid: 97.52% \ttest: 97.52%\n",
      "saving filters...\n",
      "238 : cost: 0.0453416 \ttrain: 99.032% \tvalid: 97.52% \ttest: 97.52%\n",
      "239 : cost: 0.0450951 \ttrain: 99.04% \tvalid: 97.52% \ttest: 97.52%\n",
      "240 : cost: 0.0448504 \ttrain: 99.048% \tvalid: 97.52% \ttest: 97.52%\n",
      "saving filters...\n",
      "241 : cost: 0.0446075 \ttrain: 99.06% \tvalid: 97.52% \ttest: 97.52%\n",
      "242 : cost: 0.0443663 \ttrain: 99.064% \tvalid: 97.52% \ttest: 97.53%\n",
      "243 : cost: 0.044127 \ttrain: 99.066% \tvalid: 97.52% \ttest: 97.54%\n",
      "saving filters...\n",
      "244 : cost: 0.0438895 \ttrain: 99.072% \tvalid: 97.53% \ttest: 97.54%\n",
      "245 : cost: 0.0436537 \ttrain: 99.082% \tvalid: 97.54% \ttest: 97.54%\n",
      "246 : cost: 0.0434195 \ttrain: 99.088% \tvalid: 97.53% \ttest: 97.54%\n",
      "saving filters...\n",
      "247 : cost: 0.0431871 \ttrain: 99.094% \tvalid: 97.54% \ttest: 97.54%\n",
      "248 : cost: 0.0429565 \ttrain: 99.1% \tvalid: 97.55% \ttest: 97.54%\n",
      "249 : cost: 0.0427274 \ttrain: 99.108% \tvalid: 97.55% \ttest: 97.54%\n",
      "saving filters...\n",
      "250 : cost: 0.0425001 \ttrain: 99.11% \tvalid: 97.56% \ttest: 97.55%\n",
      "251 : cost: 0.0422743 \ttrain: 99.112% \tvalid: 97.56% \ttest: 97.55%\n",
      "252 : cost: 0.0420502 \ttrain: 99.114% \tvalid: 97.56% \ttest: 97.56%\n",
      "saving filters...\n",
      "253 : cost: 0.0418277 \ttrain: 99.122% \tvalid: 97.56% \ttest: 97.57%\n",
      "254 : cost: 0.0416068 \ttrain: 99.132% \tvalid: 97.56% \ttest: 97.57%\n",
      "255 : cost: 0.0413875 \ttrain: 99.138% \tvalid: 97.58% \ttest: 97.58%\n",
      "saving filters...\n",
      "256 : cost: 0.0411697 \ttrain: 99.146% \tvalid: 97.59% \ttest: 97.58%\n",
      "257 : cost: 0.0409535 \ttrain: 99.154% \tvalid: 97.59% \ttest: 97.61%\n",
      "258 : cost: 0.0407388 \ttrain: 99.162% \tvalid: 97.6% \ttest: 97.61%\n",
      "saving filters...\n",
      "259 : cost: 0.0405257 \ttrain: 99.166% \tvalid: 97.6% \ttest: 97.6%\n",
      "260 : cost: 0.040314 \ttrain: 99.17% \tvalid: 97.6% \ttest: 97.6%\n",
      "261 : cost: 0.0401038 \ttrain: 99.174% \tvalid: 97.61% \ttest: 97.6%\n",
      "saving filters...\n",
      "262 : cost: 0.0398951 \ttrain: 99.184% \tvalid: 97.61% \ttest: 97.6%\n",
      "263 : cost: 0.0396879 \ttrain: 99.19% \tvalid: 97.61% \ttest: 97.6%\n",
      "264 : cost: 0.0394821 \ttrain: 99.194% \tvalid: 97.61% \ttest: 97.6%\n",
      "saving filters...\n",
      "265 : cost: 0.0392777 \ttrain: 99.204% \tvalid: 97.62% \ttest: 97.6%\n",
      "266 : cost: 0.0390748 \ttrain: 99.206% \tvalid: 97.62% \ttest: 97.61%\n",
      "267 : cost: 0.0388732 \ttrain: 99.214% \tvalid: 97.62% \ttest: 97.61%\n",
      "saving filters...\n",
      "268 : cost: 0.0386731 \ttrain: 99.22% \tvalid: 97.62% \ttest: 97.61%\n",
      "269 : cost: 0.0384743 \ttrain: 99.23% \tvalid: 97.64% \ttest: 97.61%\n",
      "270 : cost: 0.0382769 \ttrain: 99.248% \tvalid: 97.64% \ttest: 97.61%\n",
      "saving filters...\n",
      "271 : cost: 0.0380808 \ttrain: 99.25% \tvalid: 97.64% \ttest: 97.61%\n",
      "272 : cost: 0.0378862 \ttrain: 99.252% \tvalid: 97.63% \ttest: 97.62%\n",
      "273 : cost: 0.0376928 \ttrain: 99.272% \tvalid: 97.63% \ttest: 97.62%\n",
      "saving filters...\n",
      "274 : cost: 0.0375007 \ttrain: 99.274% \tvalid: 97.64% \ttest: 97.63%\n",
      "275 : cost: 0.03731 \ttrain: 99.278% \tvalid: 97.64% \ttest: 97.63%\n",
      "276 : cost: 0.0371205 \ttrain: 99.28% \tvalid: 97.64% \ttest: 97.63%\n",
      "saving filters...\n",
      "277 : cost: 0.0369324 \ttrain: 99.29% \tvalid: 97.65% \ttest: 97.63%\n",
      "278 : cost: 0.0367454 \ttrain: 99.294% \tvalid: 97.65% \ttest: 97.63%\n",
      "279 : cost: 0.0365598 \ttrain: 99.3% \tvalid: 97.65% \ttest: 97.64%\n",
      "saving filters...\n",
      "280 : cost: 0.0363754 \ttrain: 99.306% \tvalid: 97.65% \ttest: 97.64%\n",
      "281 : cost: 0.0361922 \ttrain: 99.31% \tvalid: 97.64% \ttest: 97.65%\n",
      "282 : cost: 0.0360103 \ttrain: 99.316% \tvalid: 97.65% \ttest: 97.65%\n",
      "saving filters...\n",
      "283 : cost: 0.0358296 \ttrain: 99.32% \tvalid: 97.65% \ttest: 97.65%\n",
      "284 : cost: 0.0356501 \ttrain: 99.322% \tvalid: 97.65% \ttest: 97.65%\n",
      "285 : cost: 0.0354718 \ttrain: 99.324% \tvalid: 97.65% \ttest: 97.65%\n",
      "saving filters...\n",
      "286 : cost: 0.0352947 \ttrain: 99.334% \tvalid: 97.65% \ttest: 97.66%\n",
      "287 : cost: 0.0351187 \ttrain: 99.334% \tvalid: 97.66% \ttest: 97.66%\n",
      "288 : cost: 0.0349439 \ttrain: 99.338% \tvalid: 97.66% \ttest: 97.66%\n",
      "saving filters...\n",
      "289 : cost: 0.0347703 \ttrain: 99.34% \tvalid: 97.66% \ttest: 97.68%\n",
      "290 : cost: 0.0345977 \ttrain: 99.342% \tvalid: 97.66% \ttest: 97.69%\n",
      "291 : cost: 0.0344264 \ttrain: 99.346% \tvalid: 97.66% \ttest: 97.69%\n",
      "saving filters...\n",
      "292 : cost: 0.0342562 \ttrain: 99.356% \tvalid: 97.66% \ttest: 97.69%\n",
      "293 : cost: 0.0340871 \ttrain: 99.358% \tvalid: 97.66% \ttest: 97.69%\n",
      "294 : cost: 0.033919 \ttrain: 99.362% \tvalid: 97.66% \ttest: 97.72%\n",
      "saving filters...\n",
      "295 : cost: 0.0337521 \ttrain: 99.368% \tvalid: 97.67% \ttest: 97.72%\n",
      "296 : cost: 0.0335863 \ttrain: 99.37% \tvalid: 97.67% \ttest: 97.73%\n",
      "297 : cost: 0.0334215 \ttrain: 99.378% \tvalid: 97.67% \ttest: 97.73%\n",
      "saving filters...\n",
      "298 : cost: 0.0332579 \ttrain: 99.388% \tvalid: 97.67% \ttest: 97.74%\n",
      "299 : cost: 0.0330952 \ttrain: 99.39% \tvalid: 97.67% \ttest: 97.74%\n",
      "300 : cost: 0.0329337 \ttrain: 99.392% \tvalid: 97.68% \ttest: 97.75%\n",
      "saving filters...\n"
     ]
    }
   ],
   "source": [
    "# Main training loop\n",
    "batch_size = 100\n",
    "epochs = 300\n",
    "check_frequency = 3\n",
    "\n",
    "train_batches = len(train_x) / batch_size\n",
    "valid_batches = len(valid_x) / batch_size\n",
    "test_batches = len(test_x) / batch_size\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print epoch+1, \":\",\n",
    "    \n",
    "    train_costs = []\n",
    "    train_accuracy = []\n",
    "    for i in range(train_batches):\n",
    "        batch_x = train_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = train_y[i*batch_size:(i+1)*batch_size]\n",
    "\n",
    "        costs = f_train(batch_x, batch_labels)\n",
    "        preds = f_test(batch_x)\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        train_costs.append(costs)\n",
    "        train_accuracy.append(acc)\n",
    "    print \"cost:\", numpy.mean(train_costs), \"\\ttrain:\", str(numpy.mean(train_accuracy)*100)+\"%\",\n",
    "    \n",
    "    valid_accuracy = []\n",
    "    for i in range(valid_batches):\n",
    "        batch_x = valid_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = valid_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        preds = f_test(batch_x)\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        valid_accuracy.append(acc)\n",
    "    print \"\\tvalid:\", str(numpy.mean(valid_accuracy)*100)+\"%\",\n",
    "    \n",
    "    test_accuracy = []\n",
    "    for i in range(test_batches):\n",
    "        batch_x = test_x[i*batch_size:(i+1)*batch_size]\n",
    "        batch_labels = test_y[i*batch_size:(i+1)*batch_size]\n",
    "        \n",
    "        preds = f_test(batch_x)\n",
    "        acc = sum(preds==batch_labels)/float(len(batch_labels))\n",
    "        \n",
    "        test_accuracy.append(acc)\n",
    "    print \"\\ttest:\", str(numpy.mean(test_accuracy)*100)+\"%\"\n",
    "    \n",
    "    if (epoch+1) % check_frequency == 0:\n",
    "        print 'saving filters...'\n",
    "        weight_filters = pil_img.fromarray(\n",
    "                tile_raster_images(\n",
    "                    W_x.get_value(borrow=True).T,\n",
    "                    img_shape=(28, 28),\n",
    "                    tile_shape=(20, 25),\n",
    "                    tile_spacing=(1, 1)\n",
    "                )\n",
    "            )\n",
    "        weight_filters.save(\"mlp_filters_%d.png\"%(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
